{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bff15a",
   "metadata": {},
   "source": [
    "## extracting the data from a public API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f521c",
   "metadata": {},
   "source": [
    "**using authentication tokens, handling pagination, understanding rate limits, and automating data extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d3341",
   "metadata": {},
   "source": [
    "### Extracting Data from an API Using the Requests Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94886e81",
   "metadata": {},
   "source": [
    "Problem : determine the popularity of different programming languages such as Python, Java, and JavaScript. We could extract data from GitHub on the languages used by popular repositories and determine the prevalence of each language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278c147",
   "metadata": {},
   "source": [
    "Representational State Transfer (REST), on the other hand, relies on HTTP as the communication protocol including the use of status codes to determine successful or failed calls. It defines data types much more loosely and uses JSON heavily, though other formats are also supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c7d42",
   "metadata": {},
   "source": [
    "Step 1 : The first API we want to call is to list all the repositories on GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87eb96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get('https://api.github.com/repositories',headers={'Accept': 'application/vnd.github.v3+json'})\n",
    "print(response.status_code)\n",
    "\n",
    "#v3 is used bcox v4 is based on graphql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8838350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "application/json; charset=utf-8\n",
      "GitHub.com\n"
     ]
    }
   ],
   "source": [
    "# extract the type of content and server details that have been returned by the API, \n",
    "print (response.encoding)\n",
    "print (response.headers['Content-Type'])\n",
    "print (response.headers['server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7d8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/search/repositories')\n",
    "print (response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39db1d",
   "metadata": {},
   "source": [
    " This code indicates that the request was correct, but the server was not able to process the request. This is because we have not provided any search query parameter as specified in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac0b68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/search/repositories',\n",
    "    params={'q': 'data_science+language:python'},\n",
    "    headers={'Accept': 'application/vnd.github.v3.text-match+json'})\n",
    "print(response.status_code)\n",
    "\n",
    "# search query is encoded to look for data_science, filter the language by Python (language:python), and combine the two (+). This constructed query is passed as the query argument q to params. We also pass the argument headers containing the Accept parameter where we specify text-match+json\n",
    "# so that the response contains the matching metadata and provides the response in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff680b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**data-science-from-scratch**: repository description - \"*code for Data Science From Scratch book*\" matched with **Data Science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**data-science-blogs**: repository description - \"*A curated list of data science blogs*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**galaxy**: repository description - \"*Data intensive science for everyone.*\" matched with **Data**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**data-scientist-roadmap**: repository description - \"*Toturials coming with the \"data science roadmap\" picture.*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**DataCamp**: repository description - \"*DataCamp data-science courses*\" matched with **data**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**dsp**: repository description - \"*data science preparation*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Kaggler**: repository description - \"*Code for Kaggle Data Science Competitions*\" matched with **Data Science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**cookiecutter-data-science**: repository description - \"*A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**PDA_Book**: repository description - \"*Code Examples Data Science using Python*\" matched with **Data Science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**kedro**: repository description - \"*A Python framework for creating reproducible, maintainable and modular data science code.*\" matched with **data science**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display  \n",
    "def printmd(string): \n",
    "    display(Markdown(string))  \n",
    "\n",
    "# list of top 5 repos\n",
    "\n",
    "for item in response.json()['items'][:10]:\n",
    "    printmd('**' + item['name'] + '**' + ': repository ' +\n",
    "            item['text_matches'][0]['property'] + ' - \\\"*' +\n",
    "            item['text_matches'][0]['fragment'] + '*\\\" matched with ' + '**' +\n",
    "            item['text_matches'][0]['matches'][0]['text'] + '**')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4781a",
   "metadata": {},
   "source": [
    "## Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123cc91",
   "metadata": {},
   "source": [
    "GitHub API implements the pagination concept where it returns only one page at a time, and in this case each page contains 30 results. The links field in the response object provides details on the number of pages in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc5e363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': {'url': 'https://api.github.com/search/repositories?q=data_science%2Blanguage%3Apython&page=2',\n",
       "  'rel': 'next'},\n",
       " 'last': {'url': 'https://api.github.com/search/repositories?q=data_science%2Blanguage%3Apython&page=34',\n",
       "  'rel': 'last'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d3aad",
   "metadata": {},
   "source": [
    "**The next field provides us with a URL to the next page, which would contain the next 30 results, while the last field provides a link to the last page, which provides an indication of how many search results there are in total.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101245e4",
   "metadata": {},
   "source": [
    "## Rate limiting\n",
    "\n",
    "To ensure that an API can continue serving all users and avoid load on their infrastructure, providers will often enforce rate limits. The rate limit specifies how many requests can be made to an endpoint in a certain time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e5c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Ratelimit-Limit 60\n",
      "X-Ratelimit-Remaining 58\n",
      "Rate Limits reset at Wed Apr 20 15:05:48 2022\n"
     ]
    }
   ],
   "source": [
    "response = requests.head(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\n",
    "print('X-Ratelimit-Limit', response.headers['X-Ratelimit-Limit'])\n",
    "print('X-Ratelimit-Remaining', response.headers['X-Ratelimit-Remaining'])\n",
    "\n",
    "# Converting UTC time to human-readable format\n",
    "import datetime\n",
    "print(\n",
    "    'Rate Limits reset at',\n",
    "    datetime.datetime.fromtimestamp(int(\n",
    "        response.headers['X-RateLimit-Reset'])).strftime('%c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccb9c6",
   "metadata": {},
   "source": [
    "X-Ratelimit-Limit indicates how many requests can be made per unit of time (one hour in this case), X-Ratelimit-Remaining is the number of requests that can still be made without violating the rate limits, and X-RateLimit-Reset indicates the time at which the rate would be reset. Itâ€™s possible for different API endpoints to have different rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901515a",
   "metadata": {},
   "source": [
    "### Extracting Twitter Data with Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fd731",
   "metadata": {},
   "source": [
    "General steps :\n",
    "    \n",
    "1. Obtaining Credentials\n",
    "2. create an app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe5eae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Host api.twitter.com\n"
     ]
    }
   ],
   "source": [
    "# installing tweepy\n",
    "# import\n",
    "\n",
    "import tweepy\n",
    "\n",
    "app_api_key = 'VMv15qXtkYFsokt1yoCc5Ia6K'\n",
    "app_api_secret_key = 'RbLjXG9ll9tRejwMAyNHNTQTdhPZ7LPhjL9WRTAsM9i6hRs8nY'\n",
    "\n",
    "\n",
    "auth = tweepy.AppAuthHandler(app_api_key, app_api_secret_key)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print ('API Host', api.host)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d867adc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Stock market these days more volatility than #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>RT @tavalez74: #FireANTS #Metaverse Museum abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @atnircapital: ATNIR Capital is a decentral...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "70  Stock market these days more volatility than #...\n",
       "93  RT @tavalez74: #FireANTS #Metaverse Museum abo...\n",
       "16  RT @atnircapital: ATNIR Capital is a decentral..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "search_term = 'cryptocurrency'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search_tweets,\n",
    "                       q=search_term,\n",
    "                       lang=\"en\").items(100)\n",
    "\n",
    "retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "df = pd.json_normalize(retrieved_tweets)\n",
    "\n",
    "df[['text']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a152d1",
   "metadata": {},
   "source": [
    "**We have successfully completed the API call and can see the text of the retrieved tweets in the previous table, which already show interesting aspects. For example, we see the use of the word RT, which indicates a retweet (where the user has shared another tweet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f89310",
   "metadata": {},
   "source": [
    "## Extracting Data from the Streaming API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d5ebf",
   "metadata": {},
   "source": [
    "Some APIs provide near real-time data, which might also be referred to as streaming data. In such a scenario, the API would like to push the data to us rather than waiting for a get request as we have been doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6a4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweepy already provides basic functionality in the StreamListener class that contains the on_data function. \n",
    "# This function is called each time a new tweet is pushed by the streaming API, \n",
    "# and we can customize it to implement logic that is specific to certain use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
