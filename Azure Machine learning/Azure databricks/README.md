The project contains Azure databricks , Azure data lake storage Gen2 , Azure data factory.

Language used : Python , Spark , Spark SQL 

Analytic tool - Power BI


ğŸŸ§  The flow of project is:

Ingest data from API -> Mount the data into ADLS Gen2 as RAW data -> Data is stored in ADLS ingest layer -> Transform this data -> data is stored in Presentation layer of ADLS Gen2 -> Using Power BI to perfrom analysis


To construct the flow of data I will use Azure data factory pipelines.


ğŸŒ©ï¸ Data Ingestion 

Steps involved in ingesting data :

â†ªï¸ Read the CSV file using the spark dataframe reader API

â†ªï¸ create a schema so that structfield can be defined.

â†ªï¸ create a dataframe to read in the csv data

â†ªï¸  Select only the required columns

â†ªï¸  Rename the columns as required

â†ªï¸  Add ingestion date and race_timestamp to the dataframe

â†ªï¸ Write data to datalake as parquet

------------------

ğŸŒ©ï¸ Databricks Workflows


â†ªï¸ Connect multiple notebooks toghether
â†ªï¸ Invoke single notebook from other notebook
â†ªï¸ Configure and run/schedule databricks jobs
